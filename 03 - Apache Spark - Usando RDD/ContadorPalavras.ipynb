{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MBA Aula 03 Apache Spark RDD.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/robsonbrandao/artigos/blob/main/03%20-%20Apache%20Spark%20-%20Usando%20RDD/ContadorPalavras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flekT6GFDN6m"
      },
      "source": [
        "# <span style=\"color:blue\">Processamento Analítico de Dados em Larga Escala</span>\n",
        "\n",
        "## <span style=\"color:blue\">Aula 03: Spark: Fundamentos e Consultas em SQL</span>\n",
        "## <span style=\"color:blue\">Apache Spark RDD: Contador de Palavras</span>\n",
        "\n",
        "\n",
        "**ICMC/USP**\n",
        "\n",
        "O objetivo deste *notebook* é mostrar uma implementação do contador de palavras usando a classe pyspark.RDD do Apache Spark RDD. O contador de palavras é um exemplo clássico de explicação da funcionalidade do modelo de programação funcional MapReduce.\n",
        "\n",
        "O detalhamento da classe `pyspark.RDD` pode ser encontrada na documentação oficial do Spark neste [link](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD). Neste notebook são explicados apenas o uso de alguns métodos para exemplificar o contador de palavras.\n",
        "\n",
        "**IMPORTANTE: O uso do *framework* Spark requer diversas configurações no ambiente de desenvolvimento para executar o *notebook*. Dado que tal complexidade foge do escopo de nossa disciplina, recomenda-se que o *notebook* seja executado na plataforma de desenvolvimento COLAB. O uso do COLAB  proporciona um ambiente de desenvolvimento pré-configurado e remove a complexidade de instalação e configuração de pacotes e *frameworks* que são utilizados na disciplina.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sO16-7-jOioq"
      },
      "source": [
        "# 1 Apache Spark Cluster"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVEgY9qKflBV"
      },
      "source": [
        "## 1.1 Instalação\n",
        "\n",
        "Neste *notebook* é criado um *cluster* Spark composto apenas por um **nó mestre**. Ou seja, o *cluster* não possui um ou mais **nós de trabalho** e o **gerenciador de cluster**. Nessa configuração, as tarefas (*tasks*) são realizadas no próprio *driver* localizado no **nó mestre**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaM-OnIjgLS2"
      },
      "source": [
        "Para que o cluster possa ser criado, primeiramente é instalado o Java Runtime Environment (JRE) versão 8. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXls3bfoglKW"
      },
      "source": [
        "# instalando Java Runtime Environment (JRE) versão 8\n",
        "%%capture\n",
        "!apt-get remove openjdk*\n",
        "!apt-get update --fix-missing\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BQzZfDYhb4j"
      },
      "source": [
        "Na sequência, é feito o *download* do Apache Spark versão 3.0.0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8a_Yv59zg3gm"
      },
      "source": [
        "# baixando Apache Spark versão 3.0.0\n",
        "%%capture\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.0.0-bin-hadoop2.7.tgz && rm spark-3.0.0-bin-hadoop2.7.tgz"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RETWX6wqhkLf"
      },
      "source": [
        "Na sequência, são configuradas as variáveis de ambiente JAVA_HOME e SPARK_HOME. Isto permite que tanto o Java quanto o Spark possam ser encontrados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZpR7NwOh2EB"
      },
      "source": [
        "import os\n",
        "\n",
        "#configurando a variável de ambiente JAVA_HOME\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "\n",
        "#configurando a variável de ambiente SPARK_HOME\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop2.7\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ql0z7Ro1iHQb"
      },
      "source": [
        "Por fim, são instalados dois pacotes da linguagem de programação Python, cujas funcionalidades são descritas a seguir.\n",
        "\n",
        "> **Pacote findspark:** Usado para ler a variável de ambiente SPARK_HOME e armazenar seu valor na variável dinâmica de ambiente PYTHONPATH. Como resultado, Python pode encontrar a instalação do Spark. \n",
        "\n",
        "> **Pacote pyspark:** PySpark é a API do Python para Spark. Ela possibilita o uso de Python, considerando que o *framework* Apache Spark encontra-se desenvolvido na linguagem de programação Scala. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oSYOwKljPf5"
      },
      "source": [
        "%%capture\n",
        "\n",
        "# instalando o pacote findspark\n",
        "!pip install -q findspark==1.4.2\n",
        "\n",
        "# instalando o pacote pyspark\n",
        "!pip install -q pyspark==3.0.0"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAaLyjPzmIwZ"
      },
      "source": [
        "## 1.2 Conexão\n",
        "\n",
        "PySpark não é adicionado ao *sys.path* por padrão. Isso significa que não é possível importá-lo, pois o interpretador da linguagem Python não sabe onde encontrá-lo. \n",
        "\n",
        "Para resolver esse aspecto, é necessário instalar o módulo `findspark`. Esse módulo mostra onde PySpark está localizado. Os comandos a seguir têm essa finalidade.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zm1pBTEmjp4"
      },
      "source": [
        "# importando o módulo findspark\n",
        "import findspark\n",
        "\n",
        "# carregando a variávels SPARK_HOME na variável dinâmica PYTHONPATH\n",
        "findspark.init()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmABUaOHv8XH"
      },
      "source": [
        "Depois de configurados os pacotes e módulos e inicializadas as variáveis de ambiente, é possível criar o objeto *SparkContext*. Esse objeto é criado com o objetivo de manipular RDDs diretamente. \n",
        "\n",
        "No comando de criação a seguir, é definido que é utilizado o próprio sistema operacional deste *notebook* como **nó mestre** por meio do parâmetro **local** do método **setMaster**. O complemento do parametro **[*]** indica que são alocados todos os núcleos de processamento disponíveis para o objeto *driver* criado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TxljJ_cwBCy"
      },
      "source": [
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "# O nó mestre é o local onde rodo\n",
        "conf = SparkConf().setMaster(\"local[*]\")\n",
        "spark = SparkContext(conf=conf)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PP2VzIYFKa3"
      },
      "source": [
        "# 2 Contador de Palavras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTe2TOXE1WGH"
      },
      "source": [
        "### 2.1 Conjunto de Dados\n",
        "\n",
        "Como conjunto de dados, é usado o arquivo `logs.txt`. Ele é um arquivo texto com 56.481 linhas de *log* de um servidor web. O arquivo fonte pode ser obtido neste ([link](https://github.com/logpai/loghub/tree/master/Apache))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytrmdSBwA_fk"
      },
      "source": [
        "O comando a seguir obtém os dados do arquivo texto `logs.txt`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxVUqvO1A9cR"
      },
      "source": [
        "# obtendo os dados do arquivo texto logs.txt\n",
        "# Arquivo de log de um servidor Web com 56K entradas\n",
        "%%capture\n",
        "!wget -q https://zenodo.org/record/3227177/files/Apache.tar.gz?download=1 -O logs.tar.gz\n",
        "!tar xf logs.tar.gz && rm -rf logs.tar.gz\n",
        "!mv Apache.log logs.txt"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wr7aAeRCBH58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77bce374-978f-4f41-8aa3-979c04463638"
      },
      "source": [
        "# abrindo o arquivo logs.txt no modo leitura (mode=\"r\") \n",
        "# exibindo os primeiros 10 registros do arquivo\n",
        "with open(file=\"logs.txt\", mode=\"r\") as fp: \n",
        "  for _ in range(0, 10): print(fp.readline())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Thu Jun 09 06:07:04 2005] [notice] LDAP: Built with OpenLDAP LDAP SDK\n",
            "\n",
            "[Thu Jun 09 06:07:04 2005] [notice] LDAP: SSL support unavailable\n",
            "\n",
            "[Thu Jun 09 06:07:04 2005] [notice] suEXEC mechanism enabled (wrapper: /usr/sbin/suexec)\n",
            "\n",
            "[Thu Jun 09 06:07:05 2005] [notice] Digest: generating secret for digest authentication ...\n",
            "\n",
            "[Thu Jun 09 06:07:05 2005] [notice] Digest: done\n",
            "\n",
            "[Thu Jun 09 06:07:05 2005] [notice] LDAP: Built with OpenLDAP LDAP SDK\n",
            "\n",
            "[Thu Jun 09 06:07:05 2005] [notice] LDAP: SSL support unavailable\n",
            "\n",
            "[Thu Jun 09 06:07:05 2005] [error] env.createBean2(): Factory error creating channel.jni:jni ( channel.jni, jni)\n",
            "\n",
            "[Thu Jun 09 06:07:05 2005] [error] config.update(): Can't create channel.jni:jni\n",
            "\n",
            "[Thu Jun 09 06:07:05 2005] [error] env.createBean2(): Factory error creating vm: ( vm, )\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7irXf7rie0S"
      },
      "source": [
        "### 2.2 Criação do RDD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzZKld7pEJz5"
      },
      "source": [
        "Existem duas formas de se criar RDDs, conforme descrito a seguir:\n",
        "\n",
        "- Paralelizando uma coleção de dados já existente no *driver*.\n",
        "\n",
        "- Referenciando um conjunto de dados armazenado em um sistema de armazenamento externo, como um sistema de arquivo compartilhado, HDFS, HBase, Cassandra, ou qualquer outra fonte de dados que ofereça suporte para o formato de entrada do Hadoop. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "O arquivo `logs.txt` é um arquivo armazenado em um sistema de armazenamento externo. Para se criar o RDD referente a esse arquivo, é utilizado o método `textFile()`. "
      ],
      "metadata": {
        "id": "QAu-T85X9xIF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKCruYrMKKuw"
      },
      "source": [
        "O comando a seguir utiliza o método `textFile()` para armazenar no RDD chamado `lines_rdd` os registros do arquivo de texto `\"logs.txt\"`. Como resultado,  `lines_rdd` consiste de um RDD de *strings*. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# criando o RDD lines_rdd\n",
        "lines_rdd = spark.textFile(\"logs.txt\")"
      ],
      "metadata": {
        "id": "aXb1LGoqUiIn"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wxv4HJTf0Zj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbd4743e-3da4-4b8a-a319-d1a8e0d200e5"
      },
      "source": [
        "# exibindo as 15 primeiras linhas de lines_rdd. Método take()\n",
        "lines_rdd.take(15)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[Thu Jun 09 06:07:04 2005] [notice] LDAP: Built with OpenLDAP LDAP SDK',\n",
              " '[Thu Jun 09 06:07:04 2005] [notice] LDAP: SSL support unavailable',\n",
              " '[Thu Jun 09 06:07:04 2005] [notice] suEXEC mechanism enabled (wrapper: /usr/sbin/suexec)',\n",
              " '[Thu Jun 09 06:07:05 2005] [notice] Digest: generating secret for digest authentication ...',\n",
              " '[Thu Jun 09 06:07:05 2005] [notice] Digest: done',\n",
              " '[Thu Jun 09 06:07:05 2005] [notice] LDAP: Built with OpenLDAP LDAP SDK',\n",
              " '[Thu Jun 09 06:07:05 2005] [notice] LDAP: SSL support unavailable',\n",
              " '[Thu Jun 09 06:07:05 2005] [error] env.createBean2(): Factory error creating channel.jni:jni ( channel.jni, jni)',\n",
              " \"[Thu Jun 09 06:07:05 2005] [error] config.update(): Can't create channel.jni:jni\",\n",
              " '[Thu Jun 09 06:07:05 2005] [error] env.createBean2(): Factory error creating vm: ( vm, )',\n",
              " \"[Thu Jun 09 06:07:05 2005] [error] config.update(): Can't create vm:\",\n",
              " '[Thu Jun 09 06:07:05 2005] [error] env.createBean2(): Factory error creating worker.jni:onStartup ( worker.jni, onStartup)',\n",
              " \"[Thu Jun 09 06:07:05 2005] [error] config.update(): Can't create worker.jni:onStartup\",\n",
              " '[Thu Jun 09 06:07:05 2005] [error] env.createBean2(): Factory error creating worker.jni:onShutdown ( worker.jni, onShutdown)',\n",
              " \"[Thu Jun 09 06:07:05 2005] [error] config.update(): Can't create worker.jni:onShutdown\"]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Passo a Passo"
      ],
      "metadata": {
        "id": "dClE7hpq_5uF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O método `flatMap` aplica uma função `func` sobre todos os elementos de um RDD e nivela os resultados gerados. \n",
        "\n",
        "No comando a seguir, aplica-se o método nativo do Python `split`, por meio da transformação do método `flatMap`, sobre os elementos em `line_rdd` para separar as linhas do arquivo `logs.txt` em palavras. Uma palavra é identificada quando se encontra um espaço em branco. "
      ],
      "metadata": {
        "id": "DXKPIvR9BCKw"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibAHKBMrgX_C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a097e3f-420c-4f11-bda8-872b3bda78ff"
      },
      "source": [
        "# Quebrar os textos em palavras\n",
        "words_rdd = lines_rdd.flatMap(lambda line: line.split(\" \"))\n",
        "words_rdd.take(7)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[Thu', 'Jun', '09', '06:07:04', '2005]', '[notice]', 'LDAP:']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "O método `map` aplica uma função `func` sobre todos os elementos de um RDD. \n",
        "\n",
        "No comando a seguir, usa-se o método `map` para produzir pares chave-valor. O método `map` faz o mapeamento de cada palavra presente no RDD words_rdd em um par chave-valor. O valor 1 é associado a cada palavra (ou chave)."
      ],
      "metadata": {
        "id": "mzZpZOyyCg4a"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "056XBbB5hNOU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cf661ec-ad75-4c70-c3de-36ded62b6a13"
      },
      "source": [
        "keyValue_rdd = words_rdd.map(lambda word: (word, 1))\n",
        "keyValue_rdd.take(7)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('[Thu', 1),\n",
              " ('Jun', 1),\n",
              " ('09', 1),\n",
              " ('06:07:04', 1),\n",
              " ('2005]', 1),\n",
              " ('[notice]', 1),\n",
              " ('LDAP:', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "O método `reduceByKey()`, quando aplicado a um RDD composto por pares chave-valor (C,V), agrega os valores V de forma que esses valores são computados dois a dois usando uma função `func` e agrupados de acordo com a chave C."
      ],
      "metadata": {
        "id": "p_1mjsWuDVQC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvEJfAnLhkVD"
      },
      "source": [
        "No comando a seguir, o método `reduceByKey()` é usado para agrupar palavras iguais, somando-se o número de vezes que cada palavra aparece."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RN7dL95whkco",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6b81110-bbbc-4e9d-fa51-75c85b1bc0d3"
      },
      "source": [
        "contaPal_rdd = keyValue_rdd.reduceByKey(lambda x, y: x + y)\n",
        "contaPal_rdd.take(7)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('09', 1793),\n",
              " ('06:07:04', 7),\n",
              " ('LDAP:', 106),\n",
              " ('SDK', 53),\n",
              " ('SSL', 53),\n",
              " ('support', 53),\n",
              " ('unavailable', 53)]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BOmiDyuq1H-"
      },
      "source": [
        "Pode ser interessante ordenar o resultado final usando o método `sortBy()`, de forma que as palavras com maior ocorrência apareçam primeiro. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "No comando a seguir, as palavras que mais aparecem aparecem primeiro, desde que o método `sortBy()` realiza a ordenação de forma decrescente. "
      ],
      "metadata": {
        "id": "poHn_nFRFZHt"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzjTEi0eq1bs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1ed48e1-c127-4451-99f7-51080deb2869"
      },
      "source": [
        "ordContaPal = contaPal_rdd.sortBy(lambda word_count: word_count[1], ascending=False)\n",
        "ordContaPal.take(7)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('[error]', 38081),\n",
              " ('2005]', 32309),\n",
              " ('[client', 31115),\n",
              " ('not', 28808),\n",
              " ('File', 20861),\n",
              " ('does', 20861),\n",
              " ('exist:', 20861)]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4ic4waXobT_"
      },
      "source": [
        "## Todos os Passos \n",
        "\n",
        "Todos os passos anteriores podem ser agrupados, conforme descrito a seguir.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFdzmRA_E7Sv"
      },
      "source": [
        "words_counts = spark.textFile(\"logs.txt\"). \\\n",
        "               flatMap(lambda line: line.split(\" \")). \\\n",
        "               map(lambda word: (word, 1)). \\\n",
        "               reduceByKey(lambda x, y: x + y). \\\n",
        "               sortBy(lambda word_count: word_count[1], ascending=False). \\\n",
        "               collect()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRw6NfNsFRsg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bf0a99b-868d-4e9b-d165-42ad635a8866"
      },
      "source": [
        "words_counts[0:7]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('[error]', 38081),\n",
              " ('2005]', 32309),\n",
              " ('[client', 31115),\n",
              " ('not', 28808),\n",
              " ('File', 20861),\n",
              " ('does', 20861),\n",
              " ('exist:', 20861)]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    }
  ]
}